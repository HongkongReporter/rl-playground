import numpy as np
import matplotlib.pyplot as plt
import gym
import time
from numba import jit
from cmath import exp
from email import policy
from functools import total_ordering
from sys import flags
from numpy import linalg
from numpy import random


# @jit optimization failed: need to address
# TODO: try to write this in C++
# it is way too slow


epsilon=0.001 # tollerance for VI

# Note: the env generated by make is random so for consistency
# among submissions it is important to use this sequence/set of seeds.
def prepFrozen():
    np.random.seed(5467)
    env = gym.make('FrozenLake-v1',desc=None,map_name=None)
    nA=env.nA
    nS=env.nS
    env.seed(48304)
    env._max_episode_steps = 1000
    return(nS,nA,env.P,env)

# You can use this smaller environment for debugging
# but your submission must use prepFrozen() above as is
def prepSmallFrozen(map_name=None):
    np.random.seed(5467)
    env = gym.make('FrozenLake-v1',map_name='4x4')
    nA=env.nA
    nS=env.nS
    env.seed(48304)
    # 4x4 should use maxsteps=250
    env._max_episode_steps = 250
    return(nS,nA,env.P,env)


# Number of states, actions, the transition model and the env
#nS,nA,P,env = prepFrozen()
nS,nA,P,env = prepFrozen()
# You need P for the Planning algorithms
# You need env for the evaluation, i.e., for env.reset() env.step() etc



# Demo of how to peek into and use P
# You can run this and inspect the printout to understand the structure
# of entries in P
def Pdemo():
    for i in range(3):
        s=np.random.randint(nS)
        a=np.random.randint(nA)
        print("at state ",s," using action ",a)
        print("Transition to: ",P[s][a])

#Pdemo()

#Value Iteration

# BellmanBackup+Greedy
@jit(nopython=True)
def BellmanBackup(env, s, V):
    actionValue=np.zeros(env.nA)
    for a in range(env.nA):
        for transp, nextState, reward, flag_DONE in env.P[s][a]:
            actionValue[a]+=transp*(reward+V[nextState])
    #Check if we are done
    return np.argmax(actionValue), np.max(actionValue)

def greedy(env, V):
    policy=np.zeros(env.nS)
    for s in range(env.nS):
        arg, value=BellmanBackup(env, s, V)
        policy[s]=arg
    return policy


# Returns policy, number of runs, effort and flag, true if converge, false not
@jit(nopython=True)
def valueIteration(env, steps_MAX):
    V=np.zeros(env.nS)
    theta=0.001
    nruns=1
    effort=0
    flag=False
    step=steps_MAX
    while(step>0):
        diff=0
        step-=1
        nruns+=1
        for state in range(env.nS):
            bestAction, bestActionValue=BellmanBackup(env, state, V)
            diff=max(diff, np.abs(bestActionValue-V[state])) #supnorm
            V[state]=bestActionValue
        if(diff<theta):
            flag=True
            break
    policy=np.zeros(env.nS)
    for s in range(env.nS):
        bestAction, bestActionValue=BellmanBackup(env, s, V)
        policy[s]=bestAction
    effort=env.nA*env.nS*nruns
    return policy, nruns, effort, flag

@jit(nopython=True)
def PolicyBellmanBackup(env, policy, V):
    V_Update=np.zeros(env.nS)
    for s in range(env.nS):
        a=policy[s]
        for transp, nextState, reward, flag_DONE in env.P[s][a]:
            V_Update[s]+=transp*(reward+V[nextState])
    return V_Update

@jit(nopython=True)
def policyIteration(env, steps_MAX):
    policy=np.zeros(env.nS)
    V=np.zeros(env.nS)
    theta=0.001
    nruns=1
    flag=False
    effort=0
    InnerItr=0
    step=steps_MAX
    while(step>0):
        step-=1
        oldPolicy=policy
        V_updated=PolicyBellmanBackup(env, policy, V)
        while(np.linalg.norm(V_updated-V, np.inf)>theta):
            InnerItr+=1
            V=PolicyBellmanBackup(env, policy, V_updated)
            V_updated=PolicyBellmanBackup(env, policy, V)
        policy=greedy(env, V) #U_0=B(V)
        if((policy==oldPolicy).all()):
            flag=True
            break
        nruns+=1
    effort=InnerItr*env.nS+env.nS*env.nA*nruns
    return policy, nruns, effort, flag

def ModifiedPolicyIteration(env, steps_MAX):
    policy=np.zeros(env.nS)
    V=np.zeros(env.nS)
    theta=0.001
    step=steps_MAX
    effort=0
    flag=False
    nruns=1
    while(step>0):
        step-=1
        for t in range(3):
            V=PolicyBellmanBackup(env, policy, V)
        policy=greedy(env, V)
        V_update=V
        V=PolicyBellmanBackup(env, policy, V)
        if(np.linalg.norm(V-V_update, np.inf)<theta):
            flag=True
            break
        nruns+=1
    effort=env.nA*env.nS*nruns+3*env.nS*nruns
    return policy, nruns, effort, flag

def evaluation(env, policy):
    env.reset()
    total_reward=0
    action=env.action_space.sample()
    for t in range(1000):
        observation, reward, done, info = env.step(action)
        action=int(policy[observation])
        total_reward+=reward
        if done:
            #print("Episode finished after {} timesteps".format(t+1))
            break
    return total_reward

def SinglePlot(env, nItr, IterationType):
    rounds=0
    reward_total=0
    policy, nruns, effort, flag=IterationType(env, nItr)
    while(rounds<500):
        rounds+=1
        reward=evaluation(env, policy)
        reward_total+=reward
    return nruns, effort, flag, reward_total/500

def plot(env, IterationType, IterationName):
    time=[]
    effort_list=[]
    value=[]
    for t in range(1000):
        nruns, effort, flag, reward=SinglePlot(env, t, IterationType)
        time.append(nruns)
        effort_list.append(effort)
        value.append(reward)
        if flag:
            break
    time=np.array(time)
    effort_list=np.array(effort_list)
    value=np.array(value)
    plt.plot(time, value)
    plt.title(IterationName)
    plt.xlabel('Iterations')
    plt.ylabel('Quality')
    plt.show()
    plt.plot(effort_list, value)
    plt.title(IterationName)
    plt.xlabel("effort")
    plt.ylabel("Quality")
    plt.show()

# Assignment 2, noise induced scenario
def NoisyBellmanBackup(env, s, V, alpha, Data):
    actionValue=np.zeros(env.nA)
    for a in range(env.nA):
        for transp, nextState, reward, flag_DONE in Data[s][a]:
            actionValue[a]+=transp*(reward+V[nextState])
    return np.argmax(actionValue), np.max(actionValue)

def NoisyValueIteration(env, alpha):
    V=np.zeros(env.nS)
    theta=0.001
    steps_MAX=1000
    nruns=1
    diff=float("inf") #difference of value iterations V_n-V_n+1
    policy=np.zeros(env.nS)
    #adding noise
    Data={s: {a: [] for a in range(env.nA)} for s in range(env.nS)}
    for s in range(env.nS):
        for a in range(env.nA):
            k=len(P[s][a])
            itr=0
            q=np.random.dirichlet(np.ones(k))
            for transp, nextState, reward, flag_DONE in env.P[s][a]:
                if(transp > 0):
                    transp=(1-alpha)*transp+alpha*q[itr]
                    Data[s][a].append((transp, nextState, reward, flag_DONE))
                    itr+=1
    #Same as VI
    while(steps_MAX>0 and diff>theta):
        steps_MAX-=1
        nruns+=1
        policy_update=np.zeros(env.nS)
        for state in range(env.nS):
            bestAction, bestActionValue=NoisyBellmanBackup(env, state, V, alpha, Data)
            diff=max(diff, np.abs(bestActionValue-V[state])) #supnorm
            V[state]=bestActionValue
            policy_update[state]=bestAction
        if(steps_MAX>0 and diff>theta):
            policy=policy_update
    return policy

def NoisyEvaluation(env, policy):
    currObs=env.reset()
    total_reward=0
    action=policy[currObs]
    for t in range(1000):
        next, reward, done, info = env.step(action)
        action=int(policy[next])
        total_reward+=reward
        if done:
            break
    return total_reward

def PlotNoise(env):
    reward_mean=np.zeros(9)
    reward_std=np.zeros(9)
    Alpha=[0, 0.1, 0.2, 0.3, 0.4, 0.5,0.6, 0.7, 0.8]
    for alpha in Alpha:
        rounds=0
        MAXROUNDS=500
        reward=np.zeros(MAXROUNDS)
        policy=NoisyValueIteration(env, alpha)
        while(rounds<MAXROUNDS):
            reward[rounds]=NoisyEvaluation(env, policy)
            rounds+=1
        reward_mean[int(alpha*10)]=np.mean(reward)
        reward_std[int(alpha*10)]=np.std(reward)
        print("The mean and std for ", alpha, " is ",reward_mean[int(alpha*10)], reward_std[int(alpha*10)])
    plt.title("VI planning under noise")
    plt.xlabel("alpha")
    plt.ylabel("mean of Reward")
    plt.errorbar(Alpha, reward_mean, yerr=reward_std, fmt='-o')
    plt.show()

# Task 3
# Implementation of learning
def explore(env, nEpisodes):
    nStates=env.nS
    nActions=env.nA
    #LData/data for saving the explored data.
    # We use three keys to access the next one,
    # the reason is simply that we hope to respect the memory padding
    LData={s: {a: {next: [] for next in range(nStates)} for a in range(nActions)} for s in range(nStates)}
    data={s: {a: {next: [] for next in range(nStates)} for a in range(nActions)} for s in range(nStates)}
    #Initialization: uniform distribution for each action
    for s in range(nStates):
        for a in range(nActions):
            for next in range(nStates):
                data[s][a][next]=[1, 0]
    for epi in range(nEpisodes):
        currState=env.reset()
        for t in range(1000):
            action=env.action_space.sample()
            observation, reward, done, info = env.step(action)
            #Update the count
            curr=data[currState][action][observation]
            data[currState][action][observation]=[curr[0]+2, curr[1]+reward]
            currState=observation #update the current step
            #encounter sink state
            if done:
            #update every actions, with pseudocounts 10, to the same sink state, however with same reward as before.
                for a in range(nActions):
                    curr=data[currState][action][observation]
                    data[currState][a][observation]=[curr[0]+100, curr[1]]
                break
    for s in range(nStates):
        for a in range(nActions):
            total_occur=0
            total_reward=0
            for next in range(nStates):
                info = data[s][a][next]
                total_occur+=info[0]
                total_reward+=info[1]

            for next in range(nStates):
                info = data[s][a][next]
                if(total_reward>0):
                    avg_reward=info[1]/total_reward
                else:
                    avg_reward=0
                LData[s][a][next]=[info[0]/total_occur, avg_reward]
    return LData

def LBellmanBackup(env, s, V, LData):
    actionValue=np.zeros(env.nA)
    for a in range(env.nA):
        for next in range(env.nS):
            curr=LData[s][a][next]
            transp=curr[0]
            reward=curr[1]
            actionValue[a]+=transp*(reward+V[next])
    #Check if we are done
    return np.argmax(actionValue), np.max(actionValue)


def LValueIteration(env, LData):
    V=np.zeros(env.nS)
    theta=0.001
    steps_MAX=1000
    nruns=1
    diff=float("inf") #difference of value iterations V_n-V_n+1
    while(steps_MAX>0 and diff>theta):
        steps_MAX-=1
        nruns+=1
        for state in range(env.nS):
            bestAction, bestActionValue=LBellmanBackup(env, state, V, LData)
            diff=max(diff, np.abs(bestActionValue-V[state])) #supnorm
            V[state]=bestActionValue
    policy=np.zeros(env.nS)
    for s in range(env.nS):
        bestAction, bestActionValue=LBellmanBackup(env, s, V, LData)
        policy[s]=bestAction
    return policy

def LEvaluation(env, policy):
    currObs=env.reset()
    total_reward=0
    action=policy[currObs]
    for t in range(1000):
        nextObs, reward, done, info = env.step(action)
        action=int(policy[nextObs])
        total_reward+=reward
        if done:
            break
    return total_reward

def LPlot(env):
    reward_mean=np.zeros(12)
    reward_std=np.zeros(12)
    Alpha=range(20000, 140000, 10000)
    itr=0
    for alpha in Alpha:
        rounds=0
        MAXROUNDS=500
        reward=np.zeros(MAXROUNDS)
        LData=explore(env, alpha)
        policy=LValueIteration(env, LData)
        while(rounds<MAXROUNDS):
            reward[rounds]=LEvaluation(env, policy)
            rounds+=1
        reward_mean[itr]=np.mean(reward)
        reward_std[itr]=np.std(reward)
        print("The mean and std for ", alpha, " is ",reward_mean[itr], reward_std[itr])
        itr+=1
    plt.title("VI planning under noise")
    plt.xlabel("alpha")
    plt.ylabel("mean of Reward")
    plt.errorbar(Alpha, reward_mean, yerr=reward_std, fmt='-o')
    plt.show()

plot(env, valueIteration, "Value Iteration")
#plot(env, policyIteration, "Policy Iteration")
#plot(env, ModifiedPolicyIteration, "Modified Policy Iteration")
#PlotNoise(env)
#LPlot(env)
